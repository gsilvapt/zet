{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my public Zettelkasten # The sole purpose of this site is to share byte-size knowledge with the world. I hope you find it useful. Blog Posts vs Zettelkasten # Blogs require a lot of effort to properly write and maintain. Zettelkasten, on the other hand, is a collection of byte-sized information which I write when I encounter something know and it helps me retain knowledge on two fronts: I'm sharing it publicly, so I have to make sure it's correct, and I can refer back to it later. How to Use the Site # This site can be used in two ways: If you'd like to see what things I have been writing about, you can check the Archive section freely. Search the website (press s on your keyboard) to find a given topic or keyword. NOTICE: This content is released under CC BY-NC-ND 3.0 license, meaning that it may be copied but must remain in identical form and that no content can be created that contains it other than fair-use citations with attribution. Copying this content for any commercial purpose is also not allowed. Licenses asides, I am not liable nor responsible for any misuse of the content or knowledge hereby written. If you like this site and would like to sponsor it, you can buying me a coffee . All the support is greatly appreciated.","title":"Home"},{"location":"#welcome-to-my-public-zettelkasten","text":"The sole purpose of this site is to share byte-size knowledge with the world. I hope you find it useful.","title":"Welcome to my public Zettelkasten"},{"location":"#blog-posts-vs-zettelkasten","text":"Blogs require a lot of effort to properly write and maintain. Zettelkasten, on the other hand, is a collection of byte-sized information which I write when I encounter something know and it helps me retain knowledge on two fronts: I'm sharing it publicly, so I have to make sure it's correct, and I can refer back to it later.","title":"Blog Posts vs Zettelkasten"},{"location":"#how-to-use-the-site","text":"This site can be used in two ways: If you'd like to see what things I have been writing about, you can check the Archive section freely. Search the website (press s on your keyboard) to find a given topic or keyword. NOTICE: This content is released under CC BY-NC-ND 3.0 license, meaning that it may be copied but must remain in identical form and that no content can be created that contains it other than fair-use citations with attribution. Copying this content for any commercial purpose is also not allowed. Licenses asides, I am not liable nor responsible for any misuse of the content or knowledge hereby written. If you like this site and would like to sponsor it, you can buying me a coffee . All the support is greatly appreciated.","title":"How to Use the Site"},{"location":"zet/","text":"2024-06-17 00:06: 10 - WordPress Reproducible Build 2024-06-12 21:57: 9 - TIL: QuerySets Over Multiple Tables May Not Return Unique Values 2024-06-06 20:08: 8 - Use bulks, please 2024-05-15 16:19: 7 - Django and Reverse M2M Relationship Filter 2024-05-13 22:54: 6 - Change Freezes Done Right 2024-05-06 14:05: 5 - Getting Started in CyberSec in Portugal 2024-05-03 22:58: 4 - Pyinstaller for Python apps bundling 2024-05-03 22:49: 3 - Proper Python Logging 2024-05-02 16:58: 2 - Updating your host's SSH keys 2024-04-27 23:30: 1 - Hello, from mkdocs","title":"Archive"},{"location":"zet/1/","text":"Hello, from mkdocs # For some time, I have been a fan of the Zettelkasten method and markdown, and had a self-proclaimed goal of building a site where I could share byte-sized notes about topics I deal with at work - software engineering, python and golang, cyber security, and other productivity related topics. After seeing what rwxrob 's Zettelkasten has evolved into, I finally found the right setup to accomplish the above. The two main reasons I am favouring mkdocs over other static site generators like Hugo are: Pure markdown support. Built-in search engine for keywords - no more need for tags or extra work to build a search engine for my own site. I still had the overhead of building the scripts to automatically create new notes and get them indexed for the main page but that was also a fun challenge to tackle.","title":"Hello, from mkdocs"},{"location":"zet/1/#hello-from-mkdocs","text":"For some time, I have been a fan of the Zettelkasten method and markdown, and had a self-proclaimed goal of building a site where I could share byte-sized notes about topics I deal with at work - software engineering, python and golang, cyber security, and other productivity related topics. After seeing what rwxrob 's Zettelkasten has evolved into, I finally found the right setup to accomplish the above. The two main reasons I am favouring mkdocs over other static site generators like Hugo are: Pure markdown support. Built-in search engine for keywords - no more need for tags or extra work to build a search engine for my own site. I still had the overhead of building the scripts to automatically create new notes and get them indexed for the main page but that was also a fun challenge to tackle.","title":"Hello, from mkdocs"},{"location":"zet/10/","text":"WordPress Reproducible Build # Had a task recently that essentially involved automating a Docker deployment of a WordPress blog. It was purely for testing purposes, so please note that most of its configuration is insecure as hell. Nonetheless, I spent 4 hours debugging an issue I thought was worth sharing and key to this task: I wanted the Docker containerization process to programmatically handle user creation and plugin installation. The idea is that we might need to frequently discard the blog, and we don't want to go through all the trouble of installing WordPress, installing specific versions of the plugins we want, and creating users with different privilege levels each time. Ultimately, we don't really care for most of that, so it's okay to use insecure defaults. So, we have a project tree that contains a folder ./plugins/vulnerable/ and some other dependency plugins that are required to use/activate in ./plugins/core . I spent 3 hours debugging why wp-cli was not installing the plugins due to an unforeseen need to escape the $ in the bash commands we were using just to get this going. PS: I am aware that using volumes to store the database and WordPress data would be better than bind mounts. For this case, this is sufficient really, and we don't want to over-engineer things 1 . For a serious project, I'd recommend using volumes. The attached link explains this better than I can. services: db: image: mysql:8.0 restart: always environment: ... volumes: - ./data/db:/var/lib/mysql wordpress: image: wordpress:6.5.2-php8.3-apache depends_on: - db restart: always ports: - 8080:80 volumes: - ./data/wordpress:/var/www/html - ./plugins:/tmp/plugins wp-cli: env_file: .env image: wordpress:cli-php8.3 user: \"33:33\" # hack for https://stackoverflow.com/questions/57136171/execute-a-plugin-installed-wp-cli-command-with-docker-compose depends_on: - wordpress volumes_from: - wordpress:rw command: > /bin/sh -c ' sleep 10; wp core install --path=\"/var/www/html\" --url=\"http://localhost:8080\" --title=\"DVWPS\" --admin_user=admin --admin_password=secret --admin_email=x@x.com; wp user create tester y@y.com --user_pass=megasecret --role=subscriber; for plugin in /tmp/plugins/**/*.zip; do wp plugin install $${plugin} --activate; done;' That $${plugin} is the difference between wp-cli being capable of installing the plugins or not. I couldn't see why the compose service was returning empty strings, even when I tried for plugin in /tmp/plugins/**/*.zip; do echo $plugin; done; and was too focused on directory permissions. In this case, it felt reasonable as the wp-cli needs to be capable of writing to directories that belong to another container in this setup. I tried a bunch of things until eventually, I tried installing one plugin I knew existed - it worked. \"Damn, this has to be an issue with the bash commands then,\" and it was... As for the user: \"33:33\" directive, I kept encountering permission issues because WordPress Dockerfile sets most directories for user www-data and the wp-cli runs with a different user. Furthermore, this is a shared bind volume, and it seems volumes_from isn't sufficient. This appears to be a known problem already, highlighted in that StackOverflow question. This fakes the running user UID and GID as www-data . https://docs.docker.com/storage/volumes/ \u21a9","title":"WordPress Reproducible Build"},{"location":"zet/10/#wordpress-reproducible-build","text":"Had a task recently that essentially involved automating a Docker deployment of a WordPress blog. It was purely for testing purposes, so please note that most of its configuration is insecure as hell. Nonetheless, I spent 4 hours debugging an issue I thought was worth sharing and key to this task: I wanted the Docker containerization process to programmatically handle user creation and plugin installation. The idea is that we might need to frequently discard the blog, and we don't want to go through all the trouble of installing WordPress, installing specific versions of the plugins we want, and creating users with different privilege levels each time. Ultimately, we don't really care for most of that, so it's okay to use insecure defaults. So, we have a project tree that contains a folder ./plugins/vulnerable/ and some other dependency plugins that are required to use/activate in ./plugins/core . I spent 3 hours debugging why wp-cli was not installing the plugins due to an unforeseen need to escape the $ in the bash commands we were using just to get this going. PS: I am aware that using volumes to store the database and WordPress data would be better than bind mounts. For this case, this is sufficient really, and we don't want to over-engineer things 1 . For a serious project, I'd recommend using volumes. The attached link explains this better than I can. services: db: image: mysql:8.0 restart: always environment: ... volumes: - ./data/db:/var/lib/mysql wordpress: image: wordpress:6.5.2-php8.3-apache depends_on: - db restart: always ports: - 8080:80 volumes: - ./data/wordpress:/var/www/html - ./plugins:/tmp/plugins wp-cli: env_file: .env image: wordpress:cli-php8.3 user: \"33:33\" # hack for https://stackoverflow.com/questions/57136171/execute-a-plugin-installed-wp-cli-command-with-docker-compose depends_on: - wordpress volumes_from: - wordpress:rw command: > /bin/sh -c ' sleep 10; wp core install --path=\"/var/www/html\" --url=\"http://localhost:8080\" --title=\"DVWPS\" --admin_user=admin --admin_password=secret --admin_email=x@x.com; wp user create tester y@y.com --user_pass=megasecret --role=subscriber; for plugin in /tmp/plugins/**/*.zip; do wp plugin install $${plugin} --activate; done;' That $${plugin} is the difference between wp-cli being capable of installing the plugins or not. I couldn't see why the compose service was returning empty strings, even when I tried for plugin in /tmp/plugins/**/*.zip; do echo $plugin; done; and was too focused on directory permissions. In this case, it felt reasonable as the wp-cli needs to be capable of writing to directories that belong to another container in this setup. I tried a bunch of things until eventually, I tried installing one plugin I knew existed - it worked. \"Damn, this has to be an issue with the bash commands then,\" and it was... As for the user: \"33:33\" directive, I kept encountering permission issues because WordPress Dockerfile sets most directories for user www-data and the wp-cli runs with a different user. Furthermore, this is a shared bind volume, and it seems volumes_from isn't sufficient. This appears to be a known problem already, highlighted in that StackOverflow question. This fakes the running user UID and GID as www-data . https://docs.docker.com/storage/volumes/ \u21a9","title":"WordPress Reproducible Build"},{"location":"zet/2/","text":"Updating your host's SSH keys # If you have servers which you connect to frequently via SSH and those servers happen to be under frequent deploys, your SSH keys might no longer work with an error similar to: <username>@<hostname>: Permission denied (publickey,password). This happens because the resulting server public key you get after the initial exchange is no longer valid as the server was destroyed and brought back up. A simple solution is to simply run ssh-keygen -R <hostname> to remove the old key and then try to connect again.","title":"Updating your host's SSH keys"},{"location":"zet/2/#updating-your-hosts-ssh-keys","text":"If you have servers which you connect to frequently via SSH and those servers happen to be under frequent deploys, your SSH keys might no longer work with an error similar to: <username>@<hostname>: Permission denied (publickey,password). This happens because the resulting server public key you get after the initial exchange is no longer valid as the server was destroyed and brought back up. A simple solution is to simply run ssh-keygen -R <hostname> to remove the old key and then try to connect again.","title":"Updating your host's SSH keys"},{"location":"zet/3/","text":"Proper Python Logging # After 3 months of working on a project from scratch, I finally understood how to setup a proper logging configuration in Python. Took me long enough but I'm glad I finally got it right. On the other hand, it's frustrating to see how little I was missing... So, here's what I had: import logging def setup_logging(debug: bool): logger.setLevel(logging.DEBUG if debug else logging.INFO) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Oddly enough, at least for me at the time, I wasn't getting any logging.info() calls getting printed to the console. However, if I provided the --debug flag to the program, it would print stuff and with the correct level call ( debug , info , error , etc). I was puzzled by this behaviour and just had to focus on other parts of the program until recently. As I am finalising the project and I wanted to add proper interaction from the program to the user, logging.info() had to work. After many hours... Here's what I was missing: import logging import sys logger = logging.getLogger(__name__) def setup_logging(debug: bool): logging.setLevel(logging.DEBUG if debug else logging.INFO, stream=sys.stdout) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Now all calls get printed. Frustrating to say the least, but often times we encounter these weird challenges where the solution is really that simple.","title":"Proper Python Logging"},{"location":"zet/3/#proper-python-logging","text":"After 3 months of working on a project from scratch, I finally understood how to setup a proper logging configuration in Python. Took me long enough but I'm glad I finally got it right. On the other hand, it's frustrating to see how little I was missing... So, here's what I had: import logging def setup_logging(debug: bool): logger.setLevel(logging.DEBUG if debug else logging.INFO) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Oddly enough, at least for me at the time, I wasn't getting any logging.info() calls getting printed to the console. However, if I provided the --debug flag to the program, it would print stuff and with the correct level call ( debug , info , error , etc). I was puzzled by this behaviour and just had to focus on other parts of the program until recently. As I am finalising the project and I wanted to add proper interaction from the program to the user, logging.info() had to work. After many hours... Here's what I was missing: import logging import sys logger = logging.getLogger(__name__) def setup_logging(debug: bool): logging.setLevel(logging.DEBUG if debug else logging.INFO, stream=sys.stdout) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Now all calls get printed. Frustrating to say the least, but often times we encounter these weird challenges where the solution is really that simple.","title":"Proper Python Logging"},{"location":"zet/4/","text":"Pyinstaller for Python apps bundling # I'm working on a project that will run on plethora of systems and for a long time I knew one of the most hated things about Python is it's dependency system and the hassle of preparing a safe environment without messing with your system's installation. There are many tools to ease this off, like poetry , pipenv , virtualenv , pyenv , etc but they all require you to still control and maintain a dependency file somewhere, install in the systems you want your application to run and so on. Sometimes all we want from our programs is to simply run. In addition, sometimes our programs will run in systems we have minimal or no control of at all. Hence, being able to run a program without worrying about it's dependencies is a nice feature. This is one of the things I really love about Go 's standard tooling, where you get a binary with everything bundled in and ready to run - dependency free. After some investigations, I found out about pyinstaller , a tool that bundles a Python application and all its dependencies into a single package. At first glance, I thought this was going to work regardless of the Python installation in the system - just a single \"binary\" users could run like anything else in the $PATH and voil\u00e1. Well, it wasn't that simple, but the other requirements were still met by this library. Despite requiring a Python installation in the system which your program supports, which I found later when I was trying to run a Python 3.12 application in a system that only had Python 3.8 installed, all dependencies are bundled and shipped together. pyinstaller docs are great and will let you be self-sufficient for your use cases - no need to explain what I need as it was really simple. One note is that pyinstaller 's will build a file for the platform you're building from (so, x86_64 linux will only work in x86_64 linux, arm will work in arm, etc) but this is a limitation I can live with. On my development machine I can generate and run the ARM builds, and the project's pipelines take care of building and publishing the binaries for the systems where they will run. The main advantage in my perspective is that everything gets bundled into a single executable and you don't need to care about environments, dependencies and what-not. Perhaps this will lead to a bigger file size, but it's a trade-off I'm willing to take for the sake of simplicity during deployments.","title":"Pyinstaller for Python apps bundling"},{"location":"zet/4/#pyinstaller-for-python-apps-bundling","text":"I'm working on a project that will run on plethora of systems and for a long time I knew one of the most hated things about Python is it's dependency system and the hassle of preparing a safe environment without messing with your system's installation. There are many tools to ease this off, like poetry , pipenv , virtualenv , pyenv , etc but they all require you to still control and maintain a dependency file somewhere, install in the systems you want your application to run and so on. Sometimes all we want from our programs is to simply run. In addition, sometimes our programs will run in systems we have minimal or no control of at all. Hence, being able to run a program without worrying about it's dependencies is a nice feature. This is one of the things I really love about Go 's standard tooling, where you get a binary with everything bundled in and ready to run - dependency free. After some investigations, I found out about pyinstaller , a tool that bundles a Python application and all its dependencies into a single package. At first glance, I thought this was going to work regardless of the Python installation in the system - just a single \"binary\" users could run like anything else in the $PATH and voil\u00e1. Well, it wasn't that simple, but the other requirements were still met by this library. Despite requiring a Python installation in the system which your program supports, which I found later when I was trying to run a Python 3.12 application in a system that only had Python 3.8 installed, all dependencies are bundled and shipped together. pyinstaller docs are great and will let you be self-sufficient for your use cases - no need to explain what I need as it was really simple. One note is that pyinstaller 's will build a file for the platform you're building from (so, x86_64 linux will only work in x86_64 linux, arm will work in arm, etc) but this is a limitation I can live with. On my development machine I can generate and run the ARM builds, and the project's pipelines take care of building and publishing the binaries for the systems where they will run. The main advantage in my perspective is that everything gets bundled into a single executable and you don't need to care about environments, dependencies and what-not. Perhaps this will lead to a bigger file size, but it's a trade-off I'm willing to take for the sake of simplicity during deployments.","title":"Pyinstaller for Python apps bundling"},{"location":"zet/5/","text":"Getting Started in CyberSec in Portugal # For the past week, I saw a few messages in one Portuguese developer community from kids coming out of college wanting to get into Cyber Security and didn't know where to start. Although there are dozens (hundreds? thousands?) resources out there about this, I truly feel they overcomplicate things and this is the right platform to make a list on some plans those folks can consider to get started. 1. Foundational Skills # I personally advise everyone to know some things about programming. Sooner or later, it will be useful for you to either write your own exploits, tools or scripts, or to simply read code and be comfortable doing so. Python is commonly used in cyber security, as well as in other areas of IT, so I guess that's a good place to start. 2. Certifications Route # There is a parallel industry for certifications in cyber security which might overwhelm those who are starting. If you wish to get started into pentesting, the eJPT from eLearnSecurity is a great, cheap starting point. Another flagship certification for getting you a job is OSCP but despite being marketed as an entry-level certification, it might not be adequate for newcomers into IT for two reasons: pricing ]nd knowledge. Build your foundation, you can take on this exam later. TCM Security has 2 certifications that can be a good deal for you: PNPT and PJPT . The internet says knowledge-wise they are superior to eJPT and others, including the OSCP, but they aren't as established in the industry as they are newer. If you have the budget they might be an option for you. 3. Self-practitioner Route # For some/most Portuguese folks, certifications are way out of budget (unless they are getting company-sponsored but I assume this will not be case as we're referring to fresh-graduates). The alternative is to get your hands in the dirt and put in the time. Start working out through TryHackMe which is great for newbies as it holds your hand and has content for newbies and specific topics. HackTheBox is also a great platform and it can be your next endeavour when you want a more challenging exercise. The next things you can do in this approach include a bunch of self-taught and self-motivated experiences. You can work your way through bug bounties, CTFs and even do security research of applications/services/libraries you use and like. Who knows what you will encounter. The cherry on top is to find one thing you like and have a vulnerability disclosure program you can securely report your findings, if any, and (maybe) get rewarded for it. 4. Writing # At this point, you should really start writing things out in the form of blog posts. Even if you haven't found a major 0-day on a big library or you aren't rich through bug bounties, I find people that write their thought process and talk about their failures way more valuable than those who don't. It's simple: you're sharing knowledge and explaining your ideas and ways of looking at things. This will be a great thing for recruiters as they can judge your capabilities through your work. I personally think people really like to flourish the experiences they have on research and bug bounties, so let me stress out how hard this is and that it is totally okay to not find anything at all . The process and learning you get out of it is 100% what matters in the end. Perhaps you didn't find anything because you didn't know about a system or protocol, but know you can study more. Then after you have some more insights you'll conclude there are no security vulnerabilities there - but at least you learnt a new protocol/service/framework and in the next project of yours you'll be better prepared. Aim to learn about those systems, and not to make money. 5. Networking # I can't stress this enough. Cyber security is a niche market within the IT field. Get to know the people in the field, attend meetups, conferences, and even submit talks to those. Introduce yourself, don't be afraid to ask questions and learn from them. From experience, everybody is super welcoming and willing to help you out. Keep in mind is mostly through networking that the greatest opportunities will come to you. Although I'm no career guru to give you any advice on anything at all, I believe these five simple, yet hardworking steps, will really get you an advantage to get you started in the field. Good luck!","title":"Getting Started in CyberSec in Portugal"},{"location":"zet/5/#getting-started-in-cybersec-in-portugal","text":"For the past week, I saw a few messages in one Portuguese developer community from kids coming out of college wanting to get into Cyber Security and didn't know where to start. Although there are dozens (hundreds? thousands?) resources out there about this, I truly feel they overcomplicate things and this is the right platform to make a list on some plans those folks can consider to get started.","title":"Getting Started in CyberSec in Portugal"},{"location":"zet/5/#1-foundational-skills","text":"I personally advise everyone to know some things about programming. Sooner or later, it will be useful for you to either write your own exploits, tools or scripts, or to simply read code and be comfortable doing so. Python is commonly used in cyber security, as well as in other areas of IT, so I guess that's a good place to start.","title":"1. Foundational Skills"},{"location":"zet/5/#2-certifications-route","text":"There is a parallel industry for certifications in cyber security which might overwhelm those who are starting. If you wish to get started into pentesting, the eJPT from eLearnSecurity is a great, cheap starting point. Another flagship certification for getting you a job is OSCP but despite being marketed as an entry-level certification, it might not be adequate for newcomers into IT for two reasons: pricing ]nd knowledge. Build your foundation, you can take on this exam later. TCM Security has 2 certifications that can be a good deal for you: PNPT and PJPT . The internet says knowledge-wise they are superior to eJPT and others, including the OSCP, but they aren't as established in the industry as they are newer. If you have the budget they might be an option for you.","title":"2. Certifications Route"},{"location":"zet/5/#3-self-practitioner-route","text":"For some/most Portuguese folks, certifications are way out of budget (unless they are getting company-sponsored but I assume this will not be case as we're referring to fresh-graduates). The alternative is to get your hands in the dirt and put in the time. Start working out through TryHackMe which is great for newbies as it holds your hand and has content for newbies and specific topics. HackTheBox is also a great platform and it can be your next endeavour when you want a more challenging exercise. The next things you can do in this approach include a bunch of self-taught and self-motivated experiences. You can work your way through bug bounties, CTFs and even do security research of applications/services/libraries you use and like. Who knows what you will encounter. The cherry on top is to find one thing you like and have a vulnerability disclosure program you can securely report your findings, if any, and (maybe) get rewarded for it.","title":"3. Self-practitioner Route"},{"location":"zet/5/#4-writing","text":"At this point, you should really start writing things out in the form of blog posts. Even if you haven't found a major 0-day on a big library or you aren't rich through bug bounties, I find people that write their thought process and talk about their failures way more valuable than those who don't. It's simple: you're sharing knowledge and explaining your ideas and ways of looking at things. This will be a great thing for recruiters as they can judge your capabilities through your work. I personally think people really like to flourish the experiences they have on research and bug bounties, so let me stress out how hard this is and that it is totally okay to not find anything at all . The process and learning you get out of it is 100% what matters in the end. Perhaps you didn't find anything because you didn't know about a system or protocol, but know you can study more. Then after you have some more insights you'll conclude there are no security vulnerabilities there - but at least you learnt a new protocol/service/framework and in the next project of yours you'll be better prepared. Aim to learn about those systems, and not to make money.","title":"4. Writing"},{"location":"zet/5/#5-networking","text":"I can't stress this enough. Cyber security is a niche market within the IT field. Get to know the people in the field, attend meetups, conferences, and even submit talks to those. Introduce yourself, don't be afraid to ask questions and learn from them. From experience, everybody is super welcoming and willing to help you out. Keep in mind is mostly through networking that the greatest opportunities will come to you. Although I'm no career guru to give you any advice on anything at all, I believe these five simple, yet hardworking steps, will really get you an advantage to get you started in the field. Good luck!","title":"5. Networking"},{"location":"zet/6/","text":"Change Freezes Done Right # In many mature companies, the product development process is often halted for a period to avoid changes that could impact product stability and, consequently, the company's business during critical times. Think of changes to the Steam store on Christmas or adjustments to the Amazon website on Black Friday. You can find a good article on this practice here . Note: Yes, I am aware that a change freeze doesn't necessarily mean stop working. On the contrary, many people still do a bunch of work during these periods; they just can't deploy that work into production. In most cases, teams deploy things into development environments and then wait for the end of the freeze to deploy everything. Despite being a good practice, I am slightly against it in the models companies use, and this note is to state what I think it should be instead under normal circumstances (there are obvious exceptions at times that might justify freezing everything): Duration : The period should be short, ideally a week or two. Anything longer is a sign of a broken process, a flawed code base, or inadequate tools. People can ignore it, sure, but the root cause of instability still exists and will impact the product regardless of the change freeze. There is an obvious exception for when the critical period extends longer, but you get the idea. In the example of Steam given above, perhaps the period is justified to be longer than 2 weeks during Christmas because it's not only on Christmas Eve people buy games - but more so during the whole holiday season. Allowing Changes : Despite typically no one being allowed to make changes, some teams' work does not really impact the product. As such, this concept should be reviewed based on two main factors: The nature of the team's work - DevOps teams can impact the CI/CD but rarely the product itself (depends on the work, obviously; they aren't going to do some database upgrades in this period). But I don't understand the problem with continuing to work on platform automation, for instance. Again, with the right balance, things should still be possible. The history of the team or tribe's responsibility causing incidents in the past . This last point is critical and is a game-changer for most. Teams that cause instability over time should be incentivized to fix their processes, tools, and/or products. Maybe this will make them rethink their technical debt, enabling them to work during these periods. Of course, this might not suffice, but at least it doesn't impact other teams' work if we take into account the other suggestions. Some may say this is a form of finger-pointing, but I find it reasonable enough to be worth it. It's a form of gamification and a way to make people think about the consequences of their processes, tools, and products (including the code base). I'm not judging those who do their best to keep things running and are hands-tied to actually fix them. Companies nowadays often throw money at problems when sometimes they should just solve the problems instead. I'm a fan of KISS, so I work wholeheartedly to make the things I work on simple and easy to work with. I'm happy when a colleague mentions that a given process, tool, or whatever is well-documented, allowing them to understand where they should be headed and get their work done. It's really painful to maintain legacy things, and I truthfully wonder if it doesn't cost more money maintaining the legacy instead of fixing things around. Perhaps the business can't stop entirely to make it work, but maybe improving things gradually is the way to go. Neither seems to happen, and then high-level management came up with change freezes, which stops everyone from continuing with the agile lifecycle.","title":"Change Freezes Done Right"},{"location":"zet/6/#change-freezes-done-right","text":"In many mature companies, the product development process is often halted for a period to avoid changes that could impact product stability and, consequently, the company's business during critical times. Think of changes to the Steam store on Christmas or adjustments to the Amazon website on Black Friday. You can find a good article on this practice here . Note: Yes, I am aware that a change freeze doesn't necessarily mean stop working. On the contrary, many people still do a bunch of work during these periods; they just can't deploy that work into production. In most cases, teams deploy things into development environments and then wait for the end of the freeze to deploy everything. Despite being a good practice, I am slightly against it in the models companies use, and this note is to state what I think it should be instead under normal circumstances (there are obvious exceptions at times that might justify freezing everything): Duration : The period should be short, ideally a week or two. Anything longer is a sign of a broken process, a flawed code base, or inadequate tools. People can ignore it, sure, but the root cause of instability still exists and will impact the product regardless of the change freeze. There is an obvious exception for when the critical period extends longer, but you get the idea. In the example of Steam given above, perhaps the period is justified to be longer than 2 weeks during Christmas because it's not only on Christmas Eve people buy games - but more so during the whole holiday season. Allowing Changes : Despite typically no one being allowed to make changes, some teams' work does not really impact the product. As such, this concept should be reviewed based on two main factors: The nature of the team's work - DevOps teams can impact the CI/CD but rarely the product itself (depends on the work, obviously; they aren't going to do some database upgrades in this period). But I don't understand the problem with continuing to work on platform automation, for instance. Again, with the right balance, things should still be possible. The history of the team or tribe's responsibility causing incidents in the past . This last point is critical and is a game-changer for most. Teams that cause instability over time should be incentivized to fix their processes, tools, and/or products. Maybe this will make them rethink their technical debt, enabling them to work during these periods. Of course, this might not suffice, but at least it doesn't impact other teams' work if we take into account the other suggestions. Some may say this is a form of finger-pointing, but I find it reasonable enough to be worth it. It's a form of gamification and a way to make people think about the consequences of their processes, tools, and products (including the code base). I'm not judging those who do their best to keep things running and are hands-tied to actually fix them. Companies nowadays often throw money at problems when sometimes they should just solve the problems instead. I'm a fan of KISS, so I work wholeheartedly to make the things I work on simple and easy to work with. I'm happy when a colleague mentions that a given process, tool, or whatever is well-documented, allowing them to understand where they should be headed and get their work done. It's really painful to maintain legacy things, and I truthfully wonder if it doesn't cost more money maintaining the legacy instead of fixing things around. Perhaps the business can't stop entirely to make it work, but maybe improving things gradually is the way to go. Neither seems to happen, and then high-level management came up with change freezes, which stops everyone from continuing with the agile lifecycle.","title":"Change Freezes Done Right"},{"location":"zet/7/","text":"Django and Reverse M2M Relationship Filter # While working on a project today, I learnt it's possible to execute reverse many-to-many relationship filters directly in Django. from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Tag(models.Model): name = models.CharField(max_length=100) author = models.ForeignKey('Author', on_delete=models.CASCADE) class Post(models.Model): title = models.CharField(max_length=100) tags = models.ManyToManyField(Tag) In a scenario you need to find all posts with tags created by user, you can do the following: post = Post.objects.filter(tags__author__name=\"author_name\") Much better and cleaner than chaining different querysets, right? SQL-wise, it also seems more efficient. The only thing that is missing there is the prefetch_related which again can be added to the queryset to avoid the N+1 problem: post = Post.objects.filter(tags__author__name=\"tag_name\").prefetch_related('tags_set__author') The addition there is the _set which is the default name for the reverse relation lookups.","title":"Django and Reverse M2M Relationship Filter"},{"location":"zet/7/#django-and-reverse-m2m-relationship-filter","text":"While working on a project today, I learnt it's possible to execute reverse many-to-many relationship filters directly in Django. from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Tag(models.Model): name = models.CharField(max_length=100) author = models.ForeignKey('Author', on_delete=models.CASCADE) class Post(models.Model): title = models.CharField(max_length=100) tags = models.ManyToManyField(Tag) In a scenario you need to find all posts with tags created by user, you can do the following: post = Post.objects.filter(tags__author__name=\"author_name\") Much better and cleaner than chaining different querysets, right? SQL-wise, it also seems more efficient. The only thing that is missing there is the prefetch_related which again can be added to the queryset to avoid the N+1 problem: post = Post.objects.filter(tags__author__name=\"tag_name\").prefetch_related('tags_set__author') The addition there is the _set which is the default name for the reverse relation lookups.","title":"Django and Reverse M2M Relationship Filter"},{"location":"zet/8/","text":"Use bulks, please # In a recent change we were performing, we needed to update our database records in a magnitude of hundreds of thousands. Did so many things before, twisted and squeeze the hell out of Django's ORM (might not be the best in the industry, but it is what it is). Our deployment pipelines were failing because the migrations would either time out or just take forever to finish, causing locks and other bottlenecks in the applications. It got to a point I just grew out of it and went with bulk_create() and bulk_update where needed. update_or_create() performs two queries under the hood, this runs one for all creates and all updates. It assumes no object or all objects exist, respectively, and there are some options to find the middle ground there too, like ignore_conflicts in case of bulk_create . This also assumes good database architecture and defaults, meaning unique constraints are properly followed and implemented in a way that will not interfere when running the bulks. Performance-wise, it went from 5 minutes to 5 seconds locally, from a never-ending change in production to 20 minutes. Add batch_size if you're concerned and want to somehow throttle the number of objects that are created/updated. Refs: https://docs.djangoproject.com/en/5.0/ref/models/querysets/#bulk-create","title":"Use bulks, please"},{"location":"zet/8/#use-bulks-please","text":"In a recent change we were performing, we needed to update our database records in a magnitude of hundreds of thousands. Did so many things before, twisted and squeeze the hell out of Django's ORM (might not be the best in the industry, but it is what it is). Our deployment pipelines were failing because the migrations would either time out or just take forever to finish, causing locks and other bottlenecks in the applications. It got to a point I just grew out of it and went with bulk_create() and bulk_update where needed. update_or_create() performs two queries under the hood, this runs one for all creates and all updates. It assumes no object or all objects exist, respectively, and there are some options to find the middle ground there too, like ignore_conflicts in case of bulk_create . This also assumes good database architecture and defaults, meaning unique constraints are properly followed and implemented in a way that will not interfere when running the bulks. Performance-wise, it went from 5 minutes to 5 seconds locally, from a never-ending change in production to 20 minutes. Add batch_size if you're concerned and want to somehow throttle the number of objects that are created/updated. Refs: https://docs.djangoproject.com/en/5.0/ref/models/querysets/#bulk-create","title":"Use bulks, please"},{"location":"zet/9/","text":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values # Today I learned that when you query multiple tables in Django, you may not get unique values. This is specially true when the query is somehow crossing over multiple tables (filtering over relationships, for instance) and then doing the joining back in Python. We have a unit test that started failing after some work and the root cause was a duplicated object, which had me scratching me head for a while. Then it clicked that this could be the use case of distinct() and when I was ready the documentation, I was right - I had see so many distinct() calls before but never thought this was the reason. Not sure what I thought really, but had me laughing at myself after figuring out the problem. While also reviewing things, I learned that we had a mistake in our unit tests that had to be fixed. References: https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.distinct","title":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values"},{"location":"zet/9/#til-querysets-over-multiple-tables-may-not-return-unique-values","text":"Today I learned that when you query multiple tables in Django, you may not get unique values. This is specially true when the query is somehow crossing over multiple tables (filtering over relationships, for instance) and then doing the joining back in Python. We have a unit test that started failing after some work and the root cause was a duplicated object, which had me scratching me head for a while. Then it clicked that this could be the use case of distinct() and when I was ready the documentation, I was right - I had see so many distinct() calls before but never thought this was the reason. Not sure what I thought really, but had me laughing at myself after figuring out the problem. While also reviewing things, I learned that we had a mistake in our unit tests that had to be fixed. References: https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.distinct","title":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values"}]}